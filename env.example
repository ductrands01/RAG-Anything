### LLM Configuration
ENABLE_LLM_CACHE=true
ENABLE_LLM_CACHE_FOR_EXTRACT=true
MAX_TOKEN_SUMMARY=700
### Chunk size for document splitting, 500~1500 is recommended
CHUNK_SIZE=1200
CHUNK_OVERLAP_SIZE=100
### Some models like o1-mini require temperature to be set to 1
TEMPERATURE=0
### Max concurrency requests of LLM
MAX_ASYNC=2
### Time out in seconds for LLM, None for infinite timeout
TIMEOUT=240

# OpenAI API Key
OPENAI_API_KEY=sk-proj-UHjqIs8eI66mPdkY1--cOC-

# LightRAG Storage
LIGHTRAG_KV_STORAGE=PGKVStorage
LIGHTRAG_VECTOR_STORAGE=PGVectorStorage
LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage

# Language for summaries (optional)
SUMMARY_LANGUAGE=en

### Redis redis://0.0.0.0:6379 for local, redis://redis:6379 for docker
REDIS_URI=redis://0.0.0.0:6379

### Number of parallel processing documents(Less than MAX_ASYNC/2 is recommended)
MAX_PARALLEL_INSERT=2
#=================[OpenAI]=================#
EMBEDDING_BINDING=openai
EMBEDDING_BINDING_API_KEY=sk-proj---cOC-
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIM=3072
MAX_EMBEDDING_TOKEN_SIZE=8192
EMBEDDING_BINDING_HOST=https://api.openai.com/v1
EMBEDDING_BATCH_NUM=32
EMBEDDING_FUNC_MAX_ASYNC=16
LLM_BINDING=openai
LLM_BINDING_API_KEY=sk-proj-UHjqIs8eI66mPdkY1--cOC-
LLM_MODEL=gpt-4o
LLM_BINDING_HOST=https://api.openai.com/v1

POSTGRES_HOST=0.0.0.0
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DATABASE=lightrag
POSTGRES_MAX_CONNECTIONS=12

NEO4J_URI=bolt://0.0.0.0:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=admin123

INPUT_DIR=input
INPUT_FILE=

TOP_K=60
COSINE_THRESHOLD=0.5